from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, SystemMessage
from langchain_groq import ChatGroq
import os
from dotenv import load_dotenv

load_dotenv()
# ModÃ¨le LLM via Groq
llm = ChatGroq(
    model="llama3-70b-8192",  # ou "mixtral-8x7b-32768" selon ton choix
    temperature=0,
    api_key=os.getenv("GROQ_API"),  # pense Ã  configurer ta clÃ© API Groq
)

# MÃ©moire conversationnelle pour suivre lâ€™historique
memory = ConversationBufferMemory(return_messages=True)

# Nouveau prompt avec historique
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="You are a medical diagnosis assistant specialized in cancer. Ask relevant questions to refine your diagnosis."),
    MessagesPlaceholder(variable_name="history"),  # correspond Ã  la mÃ©moire
    HumanMessage(content="{input}"),
])

# ChaÃ®ne avec mÃ©moire
chain = prompt | llm

def diagnostic_agent_conversation(user_input: str) -> tuple[str, bool]:
    """
    Agent conversationnel qui pose des questions pour affiner son diagnostic.
    Retourne :
        - La rÃ©ponse gÃ©nÃ©rÃ©e (str)
        - Un flag has_followup (bool) indiquant s'il reste des questions
    """
    # Ajoute le message Ã  lâ€™historique
    memory.save_context({"input": user_input}, {"output": ""})

    # RÃ©cupÃ¨re lâ€™historique + lâ€™input utilisateur
    history = memory.load_memory_variables({})["history"]

    # GÃ©nÃ¨re la rÃ©ponse
    response = chain.invoke({
        "history": history,
        "input": user_input
    })

    # Sauvegarde la rÃ©ponse dans lâ€™historique
    memory.save_context({"input": user_input}, {"output": response.content})
    # ğŸ”¥ DÃ©tection d'une phrase clÃ© pour savoir si lâ€™agent continue
    if "Do you have any other symptoms?" in response.content or \
       "Can you provide more details?" in response.content:
        has_followup = True
    else:
        has_followup = False
    return response.content, has_followup
