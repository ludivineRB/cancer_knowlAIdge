# #
# from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain.memory import ConversationBufferMemory
# from langchain_groq import ChatGroq
# from dotenv import load_dotenv
# import os

# load_dotenv()

# psychologue_system_prompt = """
# Vous √™tes un psychologue virtuel empathique et bienveillant. Votre r√¥le est :
# - d'√©couter activement l'utilisateur,
# - de reformuler ce qu'il dit pour montrer votre compr√©hension,
# - de poser des questions ouvertes pour l'aider √† explorer ses √©motions,
# - de donner des suggestions g√©n√©rales (pas de diagnostic m√©dical).

# R√®gles :
# 1. Ne donnez jamais de conseils m√©dicaux ou juridiques pr√©cis.
# 2. Encouragez l'utilisateur √† contacter un vrai professionnel si n√©cessaire.
# 3. Soyez concis, doux et rassurant.
# """

# # Configure l'API Groq
# groq_api_key = os.getenv("GROQ_API")
# llm = ChatGroq(
#     model_name="llama-3.3-70b-versatile",  # ou mixtral-8x7b
#     temperature=0.7,
#     groq_api_key=groq_api_key
# )

# # M√©moire conversationnelle pour suivre l‚Äôhistorique
# memory = ConversationBufferMemory(
#     return_messages=True
# )

# # Nouveau prompt qui inclut l‚Äôhistorique
# prompt = ChatPromptTemplate.from_messages([
#     ("system", psychologue_system_prompt),
#     MessagesPlaceholder(variable_name="history"),  # üÜï historique ici
#     ("human", "{input}")
# ])

# # Cha√Æne qui combine prompt + mod√®le + m√©moire
# chain = prompt | llm

# def create_psychologue_agent():
#     """
#     Retourne une fonction qui g√®re la conversation
#     """
#     def agent(user_input: str):
#         # Charge l‚Äôhistorique et g√©n√®re une r√©ponse
#         history = memory.load_memory_variables({})["history"]
#         response = chain.invoke({
#             "history": history,
#             "input": user_input
#         })
#         # Sauvegarde le contexte (input + output)
#         memory.save_context({"input": user_input}, {"output": response.content})
#         return response.content

#     return agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from dotenv import load_dotenv
from langdetect import detect
from deep_translator import GoogleTranslator
import os

load_dotenv()

psychologue_system_prompt = """
Vous √™tes un psychologue virtuel empathique et bienveillant. Votre r√¥le est :
- d'√©couter activement l'utilisateur,
- de reformuler ce qu'il dit pour montrer votre compr√©hension,
- de poser des questions ouvertes pour l'aider √† explorer ses √©motions,
- de donner des suggestions g√©n√©rales (pas de diagnostic m√©dical).

R√®gles :
1. Ne donnez jamais de conseils m√©dicaux ou juridiques pr√©cis.
2. Encouragez l'utilisateur √† contacter un vrai professionnel si n√©cessaire.
3. Soyez concis, doux et rassurant.
"""

# Configure l'API Groq
groq_api_key = os.getenv("GROQ_API")
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    temperature=0.7,
    groq_api_key=groq_api_key
)

# M√©moire conversationnelle
memory = ConversationBufferMemory(return_messages=True)

# Prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", psychologue_system_prompt),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# Cha√Æne compl√®te
chain = prompt | llm


def translate_to_french_if_needed(text: str):
    """D√©tecte la langue et traduit en fran√ßais si n√©cessaire"""
    detected_lang = detect(text)
    if detected_lang != "fr":
        translated_text = GoogleTranslator(source="auto", target="fr").translate(text)
        return translated_text, detected_lang
    return text, "fr"


def translate_back_if_needed(text: str, target_lang: str):
    """Traduit de fran√ßais vers la langue originale si n√©cessaire"""
    if target_lang != "fr":
        return GoogleTranslator(source="fr", target=target_lang).translate(text)
    return text


def create_psychologue_agent():
    """
    Retourne une fonction qui g√®re la conversation
    """
    def agent(user_input: str):
        # Traduire en fran√ßais si n√©cessaire
        translated_input, original_lang = translate_to_french_if_needed(user_input)

        # Charger l‚Äôhistorique et g√©n√©rer une r√©ponse
        history = memory.load_memory_variables({})["history"]
        response = chain.invoke({
            "history": history,
            "input": translated_input
        })

        # Sauvegarder le contexte (input + output en fran√ßais)
        memory.save_context({"input": translated_input}, {"output": response.content})

        # Retraduire la r√©ponse si besoin
        final_response = translate_back_if_needed(response.content, original_lang)
        return final_response

    return agent
