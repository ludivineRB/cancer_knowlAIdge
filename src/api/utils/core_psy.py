#
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os

load_dotenv()

psychologue_system_prompt = """
Vous √™tes un psychologue virtuel empathique et bienveillant. Votre r√¥le est :
- d'√©couter activement l'utilisateur,
- de reformuler ce qu'il dit pour montrer votre compr√©hension,
- de poser des questions ouvertes pour l'aider √† explorer ses √©motions,
- de donner des suggestions g√©n√©rales (pas de diagnostic m√©dical).

R√®gles :
1. Ne donnez jamais de conseils m√©dicaux ou juridiques pr√©cis.
2. Encouragez l'utilisateur √† contacter un vrai professionnel si n√©cessaire.
3. Soyez concis, doux et rassurant.
"""

# Configure l'API Groq
groq_api_key = os.getenv("GROQ_API")
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",  # ou mixtral-8x7b
    temperature=0.7,
    groq_api_key=groq_api_key
)

# M√©moire conversationnelle pour suivre l‚Äôhistorique
memory = ConversationBufferMemory(
    return_messages=True
)

# Nouveau prompt qui inclut l‚Äôhistorique
prompt = ChatPromptTemplate.from_messages([
    ("system", psychologue_system_prompt),
    MessagesPlaceholder(variable_name="history"),  # üÜï historique ici
    ("human", "{input}")
])

# Cha√Æne qui combine prompt + mod√®le + m√©moire
chain = prompt | llm

def create_psychologue_agent():
    """
    Retourne une fonction qui g√®re la conversation
    """
    def agent(user_input: str):
        # Charge l‚Äôhistorique et g√©n√®re une r√©ponse
        history = memory.load_memory_variables({})["history"]
        response = chain.invoke({
            "history": history,
            "input": user_input
        })
        # Sauvegarde le contexte (input + output)
        memory.save_context({"input": user_input}, {"output": response.content})
        return response.content

    return agent
